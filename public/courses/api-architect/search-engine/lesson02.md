# Tokenizing Text (Split) ðŸ”ª

To build an index, we break sentences into words.
This is called **Tokenizing**.
We use `.split()` to turn a string into a list.

### The Logic
```python
sentence = "I love code"
words = sentence.split(" ")
# words is ["I", "love", "code"]
```

### Your Goal
1. Create `text = "Search engines are fast"`.
2. Split it: `tokens = text.split(" ")`.
3. Print `tokens`.

### Achievement
ðŸ”ª **Parser**: You broke text into processable chunks!
